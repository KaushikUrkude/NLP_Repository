{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0_umYoeFwbd",
        "outputId": "07c7e800-3061-4432-a622-e63a8985cc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello Welcome, to my Github's repo NLP Tokenization.\n",
        "Keep checking for my new updates!!.\n",
        "Follow me and stay tuned.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bqg1PmojGGjZ"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXlvPU9YGPAZ",
        "outputId": "699e79a7-f134-41d4-ac46-3dff020df233"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, to my Github's repo NLP Tokenization.\n",
            "Keep checking for my new updates!!.\n",
            "Follow me and stay tuned.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization -> sentence to Paragraph\n"
      ],
      "metadata": {
        "id": "pC1wWYPmGwSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "tMj-HO-hGPYA"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_d1 = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "LdZxeoZwG7cJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents_d1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng806pfRHwNZ",
        "outputId": "db152e5c-7bfe-429c-81ae-a9b2f883b0d3"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sen in documents_d1:\n",
        "  print(sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZugJygNHIEaK",
        "outputId": "87ca2493-4afb-46ff-879f-2a89b7ff7d9e"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, to my Github's repo NLP Tokenization.\n",
            "Keep checking for my new updates!!.\n",
            "Follow me and stay tuned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization -> Paragraph into words\n",
        "Tokenization -> Sentence into words\n"
      ],
      "metadata": {
        "id": "yPhImBQOIdWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "htvp6McMIb6h"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5cE4I1XI6_w",
        "outputId": "57864434-a02f-4692-8789-2affe42d1b1f"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'my', 'Github', \"'s\", 'repo', 'NLP', 'Tokenization', '.', 'Keep', 'checking', 'for', 'my', 'new', 'updates', '!', '!', '.', 'Follow', 'me', 'and', 'stay', 'tuned', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##here gituhub's 's is not been tokenized properly else is good"
      ],
      "metadata": {
        "id": "KTUcgVedJHVx"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sen in documents_d1:\n",
        "  print(word_tokenize(sen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ_yvD3fJVfo",
        "outputId": "6a24fdd6-c0f7-4dbc-c64a-f0dbd90f3892"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'my', 'Github', \"'s\", 'repo', 'NLP', 'Tokenization', '.']\n",
            "['Keep', 'checking', 'for', 'my', 'new', 'updates', '!', '!', '.']\n",
            "['Follow', 'me', 'and', 'stay', 'tuned', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Wordpunct_tokenize"
      ],
      "metadata": {
        "id": "zW2T9bdwKK0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "imZX5eNuJ9Zr"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6yj3AH9KHdh",
        "outputId": "df7ff1d6-694a-431b-93e3-2508839d3a0a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'my',\n",
              " 'Github',\n",
              " \"'\",\n",
              " 's',\n",
              " 'repo',\n",
              " 'NLP',\n",
              " 'Tokenization',\n",
              " '.',\n",
              " 'Keep',\n",
              " 'checking',\n",
              " 'for',\n",
              " 'my',\n",
              " 'new',\n",
              " 'updates',\n",
              " '!!.',\n",
              " 'Follow',\n",
              " 'me',\n",
              " 'and',\n",
              " 'stay',\n",
              " 'tuned',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes sure that makes puntuation as a diffrent word. We can se the diffrence initially the split was\n",
        "1.Github's -> Github , 's\n",
        "2.Github's -> Github , ' , s"
      ],
      "metadata": {
        "id": "C_Ba9ITJKa8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "Iagdy7awK-H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "f0kTmPX6K0sA"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "Ph-KjY7GLDXo"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBsydl3WLNpw",
        "outputId": "eee6fc1d-9e92-4d2e-c5ad-07edb870d293"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'my',\n",
              " 'Github',\n",
              " \"'s\",\n",
              " 'repo',\n",
              " 'NLP',\n",
              " 'Tokenization.',\n",
              " 'Keep',\n",
              " 'checking',\n",
              " 'for',\n",
              " 'my',\n",
              " 'new',\n",
              " 'updates',\n",
              " '!',\n",
              " '!',\n",
              " '.',\n",
              " 'Follow',\n",
              " 'me',\n",
              " 'and',\n",
              " 'stay',\n",
              " 'tuned',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see \"Tokenization.\" is returned insterad of  \"Tokenization\", \".\""
      ],
      "metadata": {
        "id": "OayvvU4CO6VQ"
      }
    }
  ]
}